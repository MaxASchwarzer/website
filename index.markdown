---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: home
---
---
layout: default
title: Max Schwarzer – Curriculum Vitae
---

# About Me

I’m a final-year PhD student in Montreal looking for industry research opportunities to use large language models (LLMs) to improve reinforcement learning (RL). RL environments are the next frontier for LLMs, and using LLMs to accelerate RL is the natural culmination of my previous research.

## Contact

Email: MaxA.Schwarzer@gmail.com  
Mobile: +1-650-288-7591  
[Github](#), [LinkedIn](#), [Scholar](#)

## Education

- PhD in Computer Science, Mila, Université de Montréal, Sep. 2020 - Aug. 2023 (est.)
- Master of Science in Computer Science, Mila, Université de Montréal, Sep. 2018 - Aug. 2020
- Bachelor of Arts in Computer Science and Mathematics, Pomona College, Aug. 2014 – May 2018


## Publications

- "Bigger, Better, Faster: Human-level Atari with human-level efficiency." ICML 2023.
- "Sample Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier." Oral presentation at ICLR 2023.
- "Simplicial Embeddings in Self-Supervised Learning and Downstream Classification." Spotlight at ICLR 2023.
- "Beyond Tabula Rasa: Reincarnating Reinforcement Learning." NeurIPS 2022.
- "The Primacy Bias in Deep Reinforcement Learning." ICML 2022.
- "Pretraining Representations for Data-Efficient Reinforcement Learning." NeurIPS 2021.
- "Deep Reinforcement Learning at the Statistical Precipice." Outstanding Paper Award at NeurIPS 2021.
- "Data-Efficient Reinforcement Learning with Self-Predictive Representations." Spotlight presentation at ICLR 2021.
- "Iterated learning for emergent systematicity in VQA." Oral presentation at ICLR 2021.
- "GAIT: A Geometric Approach to Information Theory." Oral presentation at AISTATS 2020, Oral presentation at NeurIPS 2019 Workshop on Information Theory and Machine Learning.
- "Improving Text Simplification with Sentence Fusion." NAACL Workshop on Graph-Based Methods for Natural Language Processing.
- "Learning to Fail: Predicting Fracture Evolution in Brittle Material Models using Recurrent Graph Convolutional Neural Networks." Published in Computational Materials Science.
- "The Simplicity-Adequacy Tradeoff in Text Simplification." Best undergraduate paper at 2018 Southern California NLP Symposium.
- "Robust Dendritic Computations with Sparse Distributed Representations." Presented at Cosyne 2018 and OCNS 2018.
```


## Experience

- Student Researcher, Google Brain, Montreal, QC, January 2022 - June 2023
- Research Assistant, Mila, Montreal, QC, August 2018 - August 2024
- Teaching Assistant, Mila, Montreal, QC, September-December 2021
- Clinic Team Leader, Claremont Graduate University, Claremont, CA, August 2017 - May 2018
- Research Intern, Numenta, Redwood City, CA, May-August 2017 and 2018
- Summer Research Intern, Pomona College, Claremont CA, CA, May-August 2016
- Lead Data Science Intern, Kaspect, Los Angeles, CA, January-May 2016

## Publications

A list of my publications can be found [here](#).

## Awards

- FRQNT PhD Scholarship, Fonds de recherche du Québec, 2022
- Borealis AI Fellowship, Borealis AI, 2022
- John Stauffer Scholarship for Academic Merit, Pomona College, May 2018
- Phi Beta Kappa Award, Pomona College, May 2018
- Rena Gurley Archibald High Scholarship Prize, Pomona College, May 2018
- Goldwater Scholarship, 2017
- Phi Beta Kappa, 2017
- Llewellyn Bixby Mathematics Prize, 2015-16 – Pomona College
- Pomona College Scholar, Fall 2014 – May 2018
- Physical Sciences, Math and Engineering Achievement Award, 2014 – Foothill College
